> MLE Nanodegree
> Philipp Vogler
> March 2016

# P4: Train a Smartcab to Drive

## 1. Implement a basic driving agent

** In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location? **

The line action = random.choice([None, 'forward', 'left', 'right'])` makes the agent choose a random action. It makes its way through the world and to the target location by chance. Usually, it takes quite a lot of time until the target location is reached. The agent does not learn.

## 2.  Identify and update state

** Justify why you picked these set of states, and how they model the agent and its environment.**

I put everything into the state that the sensors record and that is relevant to the action decision. This includes the recommended next waypoint and the inputs about the environment ( left, light, oncoming, right).
The deadline and the time step are not essential to the decision. They also would increase the number of states significantly, if recorded. I did not include deadline and time step in the state. They are used to adjust the learning and the exploration rate.
Later I also scraped the 'left' input from the state, because it is not relevant in a right-of-way environment.

## 3. Implement Q-Learning

 ** What changes do you notice in the agent’s behavior? **

 In the beginning, the actions are still random, due to the random initialization of the Q table. The Q table is updated based solely on the rewards the agent receives for its actions. Actions are chosen in accordance with the maximum Q value of the neighboring states. No learning rate, exploration rate or discount jet.

 After about 30 runs, the agent starts to prefer certain actions in certain states. In some cases, this is useful to reach the target destination, which the basic agent does now more frequently than the random agent. In other cases, it leads to problems like deadlock situations, a strong preference for the "None-action" or clockwise loops.

 Bottomline is that there is some learning happening, but it is slow to come. Not considering the future Q values of the neighboring states often leads to an agent trapped in a local minimum.

## 4. Enhance the driving agent

** Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform? **

* Reduce the count of states/keys in the Q table
    In my fist Q table, there was the possibility for the recommendation for the next waypoint to be NONE, which is a state that the planner does not provide.
    Also, I realized that with right-of-way rules the left- sensor is quite unimportant for the decision. I removed it from the state.
    These measures reduced the number of states in the Q table significantly.

* Optimistic initialization of the Q table
    To have "realistic" start values in the Q table, I initialized them with floats from 0 to 12. This is what the maximum reward is.

* Considering the future Q value in the Q table update
    I changed the update for the Q value in the Q table in the way that it considers the future Q values of the next possible states. This improves the learning significantly. High Q values from favorable states can trickle down to other good states now.

* Introduction of epsilon to solve the exploration-exploitation dilemma
    I introduced an exploration rate that should help to cover more states in the Q table. The way it is implemented with regard to the deadline minimizes the unwanted side effect of exploration under time pressure.
    It helped in particular against the clockwise cycling of the agent.

* Introducing a discount gamma
    Considering the future states Q values helped the learning considerably. But it has to be balanced with the rewards for the action to get there. If the discount is not high enough, the chance for penalties increases. The introduction of gamma helped to handle the strong preference for the "None-action".

* Introduction of a learning rate
    The learning rate remarkably improved the speed of the agents learning. The results improve after fewer runs.

** Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? **

With the exploration rate, there is a random element in the decision process that prohibits 100% optimal behavior. Furthermore, the agent is not preventing penalties at all costs.
That said, the policy gets very good. The reward per action, as well as the average reward, improves significantly. In the order of 20 to 40 runs the average reward asymptotes at about two. The reward per action in a run fluctuates but with an upward trend.
After about 50 runs the policy only improves marginally.

figure_1.png