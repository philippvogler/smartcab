> Philipp Vogler
> MLE Nanodegree

# P4: Train a Smartcab to Drive

## 1. Implement a basic driving agent

** In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location? **

The line action = random.choice([None, 'forward', 'left', 'right'])` makes the driver choose a random action. It makes its way through the world and to the location by chance. Usually it takes quite a lot of time until the location is reached.

## 2.  Identify and update state

** Justify why you picked these set of states, and how they model the agent and its environment.**

## 3. Implement Q-Learning

 ** What changes do you notice in the agent’s behavior? **

 * pat
 * right loops
 * long learning periods

## 4. Enhance the driving agent

** Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform? **

* reduce the count of states/keys in the q table
    In my fist Q table there was the possibility for the recommendation for the next waypoint to be NONE, which is a state that the planner does not provide.
    Also I realized that with right-of-way rules the left- sensor is quit unimportant for the decision.
* introduce a learning rate
    The learning rate really improved the learning of the agent. It help especially against the clockwise cycling.

* introduce a gamma to solve the exploration exploitation dilemma
    The exploitation rate helps to cover more states. The way it is implemented with regard to the deadline, helps to minimize the side unwanted side effect of exploration under time pressure.

* set the initial values of the q table in regard of the rewards.
  rewards for moving arround is up to 2 points. 
* implementing a performance measure


** Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? **

* the primary goal is to follow the next_waypoint recommendations without violating any traffic rules
